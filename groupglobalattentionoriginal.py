# -*- coding: utf-8 -*-
"""GroupGlobalAttentionOriginal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1616KXLhbaBdwrHCIdibuee6kWvzTixmc
"""

!pip install transformers[torch]
!pip install datasets
!pip install bertviz

import torch
import torch.nn.functional as F
import torch.nn as nn
import random

from transformers import  AutoTokenizer
# from torch import nn
from transformers import AutoConfig # To load the config.jon file associted with the bert-base-uncased checkpoint.

from transformers import AutoTokenizer
from bertviz.transformers_neuron_view import BertModel
from bertviz.neuron_view import show

from torch import nn
from transformers import AutoConfig

device = "cuda" if torch.cuda.is_available() else "cpu"
device
torch.cuda.device_count()

model_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = BertModel.from_pretrained(model_ckpt)
text = "time flies like an arrow"
show(model, "bert", tokenizer, text, display_mode="light", layer=0, head=8)

"""#WORD TOKENIZATION"""

model_ckpt = "bert-base-uncased"
text ="time flies like an arrow. i am a boy."
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True)
print(inputs.input_ids)

tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])
print(tokens)

config = AutoConfig.from_pretrained(model_ckpt)
config

"""#GROUP GLOBAL ATTENTION"""

class GroupGlobalAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, num_groups, group_length, dropout_rate=0.1):
        super(GroupGlobalAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_groups = num_groups
        self.group_length = group_length
        self.head_dim = embed_dim // num_heads
        self.dropout = nn.Dropout(dropout_rate)

        # Local Attention Layer
        self.local_attention_layer = LocalAttentionLayer(embed_dim, num_heads, dropout_rate)

        # Global Attention Layer
        self.global_attention_layer = GlobalAttentionLayer(embed_dim, num_heads, dropout_rate)

        # Combine the outputs
        self.alpha = nn.Parameter(torch.randn(1))
        self.beta = nn.Parameter(torch.randn(1))

    def forward(self, x):
        # Step 1: Grouping
        batch_size, seq_len, embed_dim = x.size()
        lg = self.group_length
        m = self.num_groups
        x = x.view(batch_size, -1, lg, embed_dim)

        # Step 2: Local Attention
        local_attention_output = self.local_attention_layer(x)

        # Step 3: Summarize Groups
        summarized_group_output = local_attention_output.max(dim=2).values

        # Step 4: Global Attention
        global_attention_output = self.global_attention_layer(summarized_group_output)

        # Step 5: Combine Local and Global Attention
        global_attention_output_expanded = global_attention_output.unsqueeze(2).expand(-1, -1, lg, -1)
        combined_output = self.alpha * local_attention_output + self.beta * global_attention_output_expanded

        # Step 6 & 7: Final Output
        batch_size, num_groups, _, embed_dim = combined_output.size()
        final_output = combined_output.view(batch_size, -1, embed_dim)

        return final_output

class LocalAttentionLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):
        super(LocalAttentionLayer, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout = nn.Dropout(dropout_rate)

        # Linear transformations for queries, keys, and values
        self.linear_q = nn.Linear(embed_dim, embed_dim)
        self.linear_k = nn.Linear(embed_dim, embed_dim)
        self.linear_v = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        batch_size, num_groups, lg, embed_dim = x.size()
        h = self.num_heads

        # Linear projections for queries, keys, and values
        queries = self.linear_q(x)
        keys = self.linear_k(x)
        values = self.linear_v(x)

        # Split heads for multi-head attention
        queries = queries.view(batch_size, num_groups, lg, h, self.head_dim).transpose(2, 3)
        keys = keys.view(batch_size, num_groups, lg, h, self.head_dim).transpose(2, 3)
        values = values.view(batch_size, num_groups, lg, h, self.head_dim).transpose(2, 3)

        # Compute attention scores
        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        # Weighted sum of values using attention scores
        local_attention_output = torch.matmul(attention_probs, values)

        # Concatenate heads and reshape
        local_attention_output = local_attention_output.transpose(2, 3).contiguous().view(batch_size, num_groups, lg, -1)

        return local_attention_output

class GlobalAttentionLayer(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):
        super(GlobalAttentionLayer, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout = nn.Dropout(dropout_rate)

        # Linear transformations for queries, keys, and values
        self.linear_q = nn.Linear(embed_dim, embed_dim)
        self.linear_k = nn.Linear(embed_dim, embed_dim)
        self.linear_v = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        batch_size, num_groups, embed_dim = x.size()

        # Linear projections for queries, keys, and values
        queries = self.linear_q(x)
        keys = self.linear_k(x)
        values = self.linear_v(x)

        # Compute attention scores
        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        # Weighted sum of values using attention scores
        global_attention_output = torch.matmul(attention_probs, values)

        return global_attention_output

# %%time
# # Define the parameters
# batch_size = 2
# seq_len = 100000
# embed_dim = 512
# num_heads = 8
# num_groups = 1000
# group_length = seq_len // num_groups
# dropout_rate = 0.1
# RANDOM_SEED = 42


# # Create an instance of GroupGlobalAttention
# model = GroupGlobalAttention(embed_dim, num_heads, num_groups, group_length, dropout_rate)

# # Generate some dummy input
# torch.manual_seed(seed=RANDOM_SEED)
# x = torch.rand(batch_size, seq_len, embed_dim)

# # Pass the input through the model
# output = model(x)

# # Print the shape of the output
# print("Output Shape:", output.shape)

# def feed_forward_network(d_model, hidden_dim):
#     return nn.Sequential(
#         nn.Linear(d_model, hidden_dim),
#         nn.ReLU(),
#         nn.Linear(hidden_dim, d_model)
#     )

"""#ENCODER BLOCK"""

class EncoderBlock(nn.Module):
    def __init__(self, d_model, num_heads, num_groups, group_length, hidden_dim, dropout_rate=0.1):
        super(EncoderBlock, self).__init__()

        self.group_global_attention = GroupGlobalAttention(d_model, num_heads, num_groups, group_length, dropout_rate)
        self.ffn = nn.Sequential(
                nn.Linear(d_model, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, d_model)
                )

        self.dropout1 = nn.Dropout(dropout_rate)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.layernorm1 = nn.LayerNorm(d_model)
        self.layernorm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        gga_output = self.group_global_attention(x)
        gga_output = self.dropout1(gga_output)
        gga_output = self.layernorm1(x + gga_output)

        ffn_output = self.ffn(gga_output)
        ffn_output = self.dropout2(ffn_output)
        output = self.layernorm2(gga_output + ffn_output)

        return output

"""#POSITIONAL EMBEDDING"""

class Embeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size,
                                             config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
                                                config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        # Create position IDs for input sequence
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        # Create token and position embeddings
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        # Combine token and position embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

"""#GROUPED GLOBAL TRANSFORMER"""

class TransformerEncoder(nn.Module):
    def __init__(self, config):
      super().__init__()
      self.embeddings = Embeddings(config)
      self.layers = nn.ModuleList([EncoderBlock(d_model = config.hidden_size, num_heads = config.num_attention_heads,num_groups=config.num_group, group_length=config.group_length, hidden_dim = 2048, dropout_rate=0.1)
                                   for _ in range(config.num_hidden_layers)])

    def forward(self, x):
        x = self.embeddings(x)
        for layer in self.layers:
            x=layer(x)
        return x#,attention_weights

"""#CALL THE TRANSFORMER ENCODER FUNCTION"""

config.num_group=1
config.group_length = 13
encoder = TransformerEncoder(config)
x= encoder(inputs.input_ids)
x.size()

"""#ADDING A CLASSIFICATION HEAD FOR TEXT CLASSIFICATION TASK"""

class TransformerForSequenceClassification(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = TransformerEncoder(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, x):
        x = self.encoder(x)[:,0,:]#select hidden state of [CLS] token
        x = self.dropout(x)
        x = self.classifier(x)
        return x

"""#Call the Transformer Encoder Function with the classifer class"""

config.num_labels = 3
encoder_classifier = TransformerForSequenceClassification(config)
encoder_classifier(inputs.input_ids).size()

# Define the parameters
d_model = 512
num_heads = 8
num_groups = 2
group_length = 100
hidden_dim = 2048
dropout_rate = 0.1
RANDOM_SEED = 42

# Create an instance of EncoderBlock
encoder_block = EncoderBlock(d_model, num_heads, num_groups, group_length, hidden_dim, dropout_rate)

# Prepare some input tensors
batch_size = 1
seq_len = 200
torch.manual_seed(seed=RANDOM_SEED)
input_data = torch.rand(batch_size, seq_len, d_model)

# Pass the input data through the EncoderBlock
output = encoder_block(input_data)

# Print the shape of the output
print("Output Shape:", output.shape)